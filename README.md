# A/B Testing
Hi, I'm Hoon. I'm also interested in a variety of A/B tests using statistical methods. An A/B test is a randomized experiment used to compare two versions (A and B) of a product, feature, or design to determine which performs better. It splits users into two groups, exposing each to one version, and measures a specific metric (e.g., conversion rate or click-through rate) to evaluate performance. The goal is to make data-driven decisions by testing changes in a controlled and statistically significant way.

Additionally, I explore Bayesian methods for A/B testing, where prior beliefs about a metric (e.g., conversion rate) are updated with data to calculate posterior distributions. This approach allows for more flexible and robust decision-making, particularly when data is sparse or uncertain.

This repository is dedicated to my practice and exploration of various A/B test methodologies, including both classical and Bayesian approaches. Thank you!
